{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ライブラリ読み込み\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, glob, pickle, time, gc, copy, sys\nimport pandas_profiling as pdp\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\n# import tensorflow as tf\n# AUTOTUNE = tf.data.experimental.AUTOTUNE\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 100) # 表示できる表の列数","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# trainファイルを読み込む\ndf_train = pd.read_csv(\"../input/ai-medical-contest-2021/train.csv\")\nprint(\"df_train.shape\", df_train.shape) # シェイプ = (行数, 列数)を表示する\n\n# testファイルを読み込む\ndf_test = pd.read_csv(\"../input/ai-medical-contest-2021//test.csv\")\nprint(\"df_test.shape\", df_test.shape) # シェイプ = (行数, 列数)を表示する\n\n# submissionファイルを読み込む\ndf_sub = pd.read_csv(\"../input/ai-medical-contest-2021//sample_submission.csv\")\nprint(\"df_sub.shape\", df_sub.shape) # シェイプ = (行数, 列数)を表示する\n\n# ECGデータのpathの列を追加.\ndf_train['path'] = df_train['Id'].apply(lambda x: \"../input/ai-medical-contest-2021/ecg/{}.npy\".format(x))\ndf_test['path'] = df_test['Id'].apply(lambda x: \"../input/ai-medical-contest-2021/ecg/{}.npy\".format(x))\nprint(df_train['path'][0]) # path列の0行目を表示\n\n# trainとtestを連結する\ndf_traintest = pd.concat([df_train, df_test]).reset_index(drop=True) # reset_index: 行のindexをリセットする\nprint(df_traintest.shape)\n\ncol_target = 'target' # ターゲットの列\ncol_index = 'Id' # idの列\nprint(\"rate of positive: {:.6f}\".format(df_train[col_target].mean())) # targetが1である割合\n\n# 各列の基本情報を表示\n# 解析対象はtrain+test\n# 列名, 型, nanの数, uniqueな値の数, 実際の値の一部, を表示する\ndf_tmp = df_traintest  # 解析するDataFrameを指定\nfor i, col in enumerate(df_tmp.columns): # 各列(column)について\n    col_name = col + \" \" * (22 - len(col)) # カラム名, 見た目上の整形のためにスペースを加える\n    type_name = \"{}\".format(df_tmp[col].dtype) # 型名\n    type_name = type_name + \" \" * (8 - len(type_name)) # 見た目上の整形のためにスペースを加える\n    num_unique = len(df_tmp[col].unique()) # ユニークな値の数\n    num_nan = pd.isna(df_tmp[col]).sum() # nanの数\n    col_head = \"{}\".format(df_tmp[col].unique()[:5].tolist())[:40] # 実際の値の一部\n    print(\"{:4d}: {} dtype: {} unique: {:8d}, nan: {:6d}, 実際の値: {}\".format(\n        i, col_name, type_name, len(df_tmp[col].unique()), num_nan, col_head)) # 表示する\n    \n# カテゴリ変数をラベルエンコーディングする (数値に置き換える).\ndf_traintest['sex'] = df_traintest['sex'].replace('female', 0) # femaleに0を代入\ndf_traintest['sex'] = df_traintest['sex'].replace('male', 1) # maleに1を代入\ndf_traintest['sex'] = df_traintest['sex'].astype(int) # 型を整数に変換\n\ndf_traintest['label_type'] = df_traintest['label_type'].replace('human', 0) # humanに0を代入\ndf_traintest['label_type'] = df_traintest['label_type'].replace('auto', 1) # autoに1を代入\ndf_traintest['label_type'] = df_traintest['label_type'].astype(int) # 型を整数に変換\n\n# train と test を再度切り分ける\ndf_train = df_traintest.iloc[:len(df_train)]\ndf_test = df_traintest.iloc[len(df_train):].reset_index(drop=True)\n\n# 全てのECGデータを読み込む\necg_train = np.zeros([len(df_train), 800, 12], np.float32) # trainの心電図データの代入先. shape=(データ数, 時間方向, 12誘導)\nfor i in range(len(df_train)): # 全てのtrain dataについて\n    path_tmp = df_train['path'][i] # i行目の心電図データのpath\n    ecg_tmp = np.load(path_tmp) # i行目の心電図データ\n    ecg_train[i] = ecg_tmp # 読み込んだ心電図データをecg_trainのi行目に代入\n\necg_test = np.zeros([len(df_test), 800, 12], np.float32) # testの心電図データの代入先. shape=(データ数, 時間方向, 12誘導)\nfor i in range(len(df_test)): # 全てのtest dataについて\n    path_tmp = df_test['path'][i] # i行目の心電図データのpath\n    ecg_tmp = np.load(path_tmp) # i行目の心電図データ\n    ecg_test[i] = ecg_tmp # 読み込んだ心電図データをecg_trainのi行目に代入\nprint(\"ecg_train.shape: {}\".format(ecg_train.shape))\nprint(\"ecg_test.shape: {}\".format(ecg_test.shape))\n\n# target情報をnumpy形式に変換\ntarget_train = df_train[col_target].values.astype(np.int) # pandas.Seriesからnp.ndarrayへ変換\nprint(\"target_train.shape: {}\".format(target_train.shape))","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"df_train.shape (2000, 5)\ndf_test.shape (8000, 4)\ndf_sub.shape (8000, 2)\n../input/ai-medical-contest-2021/ecg/81ac15cb8d3be42e3d0ccdea36176183.npy\n(10000, 6)\nrate of positive: 0.300000\n   0: Id                     dtype: object   unique:    10000, nan:      0, 実際の値: ['81ac15cb8d3be42e3d0ccdea36176183', '92\n   1: target                 dtype: float64  unique:        3, nan:   8000, 実際の値: [1.0, 0.0, nan]\n   2: age                    dtype: int64    unique:       92, nan:      0, 実際の値: [80, 59, 29, 75, 50]\n   3: sex                    dtype: object   unique:        2, nan:      0, 実際の値: ['female', 'male']\n   4: label_type             dtype: object   unique:        2, nan:      0, 実際の値: ['human', 'auto']\n   5: path                   dtype: object   unique:    10000, nan:      0, 実際の値: ['../input/ai-medical-contest-2021/ecg/8\necg_train.shape: (2000, 800, 12)\necg_test.shape: (8000, 800, 12)\ntarget_train.shape: (2000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# クロスバリデーションを行うためにデータを5分割する\n# 4つを学習に用い、1つを検証に要する。これを5回繰り返す。\nfolds = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(\n    np.arange(len(df_train)), \n    y=df_train[col_target]) # 各foldターゲットのラベルの分布がそろうようにする = stratified K fold\n)\n\n# fold 0の学習データと検証データの分割\nfold = 0 # fold 0 についての学習を行う\n\n# このfoldにおける学習データと検証データの切り分け\nX_train = ecg_train[folds[fold][0]] # 学習データの入力データを抽出\ny_train = target_train[folds[fold][0]] # 学習データの正解データを抽出\nX_valid = ecg_train[folds[fold][1]] # 検証データの入力データを抽出\ny_valid = target_train[folds[fold][1]] # 検証データの正解データを抽出\nprint(\"X_train.shape: {}, X_valid.shape: {}\".format(X_train.shape, X_valid.shape))\nprint(\"y_train.shape: {}, y_valid.shape: {}\".format(y_train.shape, y_valid.shape))","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"X_train.shape: (1600, 800, 12), X_valid.shape: (400, 800, 12)\ny_train.shape: (1600,), y_valid.shape: (400,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tensorflow(baseline)","metadata":{}},{"cell_type":"markdown","source":"# modelにdataを流すためのdatasetを構築する\nBATCH_SIZE = 64 # ミニバッチに含めるデータの数\ndef augment_fn(X, y):\n    \"\"\"\n    augmentation (データ水増し)を設定する\n    \"\"\"\n    X_new = tf.image.random_crop(X, (700,12)) # 時間方向に800 timepointからrandomに700 timepointを切り出す\n    return (X_new, y)\n    \n# train dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices(( # np\n    X_train, # 入力データ\n    y_train, # 正解データ\n))\ntrain_dataset = train_dataset.shuffle(len(train_dataset), reshuffle_each_iteration=True) # 学習中にデータをシャッフルすることを指定する\ntrain_dataset = train_dataset.map(augment_fn, num_parallel_calls=AUTOTUNE) # augmentationの適用\ntrain_dataset = train_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する\n\n# valid dataset\nvalid_dataset = tf.data.Dataset.from_tensor_slices((\n    X_valid, # 入力データ\n    y_valid, # 正解データ\n))\nvalid_dataset = valid_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する (シャッフルはしない)\n\n# test dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    ecg_test, # 入力データ\n    np.zeros(len(ecg_test)), # 正解データ (testに正解データないためダミーデータ)\n))\ntest_dataset = test_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する (シャッフルはしない)\n\n\n# datasetの読み込みテスト\necg_batch, target_batch = next(iter(train_dataset)) # 試しにミニバッチを読み込む\nprint(\"train ecg_batch.shape: {}\".format(ecg_batch.shape))\nprint(\"train target_batch.shape: {}\".format(target_batch.shape))\necg_batch, target_batch = next(iter(valid_dataset)) # 試しにミニバッチを読み込む\nprint(\"valid ecg_batch.shape: {}\".format(ecg_batch.shape))\nprint(\"valid target_batch.shape: {}\".format(target_batch.shape))","metadata":{}},{"cell_type":"markdown","source":"# deep learning modelの作成\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\ndef get_model(input_shape=(800, 12)):\n    model = tf.keras.models.Sequential([ # レイヤーのリストからモデルを構築する\n        tf.keras.Input(shape=input_shape), # 入力の形状の指定. shape=(時間軸, 12誘導)\n        # block1\n        tf.keras.layers.Conv1D(64, 7), # 時間方向の1次元畳み込みレイヤー. 32=出力チャネル数, 7=カーネルサイズ\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Activation('relu'), \n        tf.keras.layers.MaxPool1D(2), \n        # block2\n        tf.keras.layers.Conv1D(128, 3, strides=2),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Activation('relu'), \n        # block3\n        tf.keras.layers.Conv1D(256, 3, strides=2),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Activation('relu'),\n        # pooling\n        tf.keras.layers.GlobalAveragePooling1D(), # 時間方向のglobal pooling\n        # 最終レイヤー\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    return model\n\nmodel = get_model()\nmodel.summary()","metadata":{}},{"cell_type":"markdown","source":"# モデルのコンパイル (学習条件の設定)\nmodel = get_model(input_shape=(None, 12)) # 時間軸の入力長さNone=可変にして再度model構築\nmodel.compile(optimizer='adam', # オプティマイザーにAdamを指定\n              loss='binary_crossentropy', # 損失関数にbinary crossentropyを指定\n              metrics=['AUC'], # 評価関数にAUCを指定\n             )\n# モデルの保存方法の指定\ncheckpoint_filepath = \"weight_fold{}.ckpt\".format(fold)\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath, # 保存path\n    save_weights_only=True, # 重みのみを保存\n    monitor='val_auc', # validataionのAUCの値に基づいて重みを保存する\n    mode='max', # validataionのAUCが最大となった時重みを保存する\n    save_best_only=True # AUCが改善したときのみ保存する\n)","metadata":{}},{"cell_type":"markdown","source":"# fold1-4についても学習を行う\n# for loopの中身は上記のfold 0での処理と同様です\nfor fold in range(5):\n    print(\"fold: {}\".format(fold))\n    X_train = ecg_train[folds[fold][0]] # 学習データの入力データを抽出\n    y_train = target_train[folds[fold][0]] # 学習データの正解データを抽出\n    X_valid = ecg_train[folds[fold][1]] # 検証データの入力データを抽出\n    y_valid = target_train[folds[fold][1]] # 検証データの正解データを抽出\n    print(\"len train: {}. len valid: {}\".format(len(X_train), len(X_valid)))\n\n    # train dataset\n    train_dataset = tf.data.Dataset.from_tensor_slices((\n        X_train, # 入力データ\n        y_train, # 正解データ\n    ))\n    train_dataset = train_dataset.shuffle(len(train_dataset), reshuffle_each_iteration=True) # 学習中にデータをシャッフルすることを指定する\n    train_dataset = train_dataset.map(augment_fn, num_parallel_calls=AUTOTUNE) # augmentationの適用\n    train_dataset = train_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する\n\n    # valid dataset\n    valid_dataset = tf.data.Dataset.from_tensor_slices((\n        X_valid, # 入力データ\n        y_valid, # 正解データ\n    ))\n    valid_dataset = valid_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する (シャッフルはしない)\n\n    # model構築\n    model = get_model(input_shape=(None, 12))\n    # モデルのコンパイル (学習条件の設定)\n    model.compile(optimizer='adam', # オプティマイザーにAdamを指定\n                  loss='binary_crossentropy', # 損失関数にbinary crossentropyを指定\n                  metrics=['AUC'], # 評価関数にAUCを指定\n                 )\n    # モデルの保存方法の指定\n    checkpoint_filepath = \"weight_fold{}.ckpt\".format(fold)\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_filepath, # 保存path\n        save_weights_only=True, # 重みのみを保存\n        monitor='val_auc', # validataionのAUCの値に基づいて重みを保存する\n        mode='max', # validataionのAUCが最大となった時重みを保存する\n        save_best_only=True # AUCが改善したときのみ保存する\n    )\n    # モデルの学習\n    model.fit(\n        train_dataset, # 学習に用いるdataset\n        validation_data=valid_dataset, # 検証に用いるdataset\n        callbacks=[model_checkpoint_callback], # モデル保存方法の指定\n        epochs=16, # epoch数 (1epoch=すべての画像を1回ずつ学習に利用する)\n    )","metadata":{}},{"cell_type":"markdown","source":"# クロスバリデーションのAUCを計算する\npreds_valid = np.zeros(len(df_train), np.float32) # 予測結果の代入先\nfor fold in range(5): # 各foldについて\n    print(\"fold: {}\".format(fold))\n    # valid dataset\n    X_valid = ecg_train[folds[fold][1]] # 検証データの入力データを抽出\n    y_valid = target_train[folds[fold][1]] # 検証データの正解データを抽出\n    valid_dataset = tf.data.Dataset.from_tensor_slices((\n        X_valid, # 入力データ\n        y_valid, # 正解データ\n    ))\n    valid_dataset = valid_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する (シャッフルはしない)\n\n    # 予測\n    checkpoint_filepath = \"weight_fold{}.ckpt\".format(fold)\n    model.load_weights(checkpoint_filepath) # 最もvalid AUCが高かったエポックの重みを読み込む\n    pred_valid = model.predict(valid_dataset) # 予測の実行\n    preds_valid[folds[fold][1]] = pred_valid[:,0] # 予測結果の代入\n\nvalid_auc = metrics.roc_auc_score(df_train[col_target], preds_valid)\nprint(\"CV: {:.6f}\".format(valid_auc))","metadata":{}},{"cell_type":"markdown","source":"# test dataに対する予測\npreds_test = np.zeros([5, len(df_test)], np.float32) # 予測結果の代入先\nfor fold in range(5): # 各foldについて\n    print(\"fold: {}\".format(fold))\n    # valid dataset\n    X_valid = ecg_train[folds[fold][1]] # 検証データの入力データを抽出\n    y_valid = target_train[folds[fold][1]] # 検証データの正解データを抽出\n    valid_dataset = tf.data.Dataset.from_tensor_slices((\n        X_valid, # 入力データ\n        y_valid, # 正解データ\n    ))\n    valid_dataset = valid_dataset.batch(BATCH_SIZE) # データセットをミニバッチ化してモデルに入力することを指定する (シャッフルはしない)\n\n    # 予測\n    checkpoint_filepath = \"weight_fold{}.ckpt\".format(fold)\n    model.load_weights(checkpoint_filepath) # 最もvalid AUCが高かったエポックの重みを読み込む\n    pred_test = model.predict(test_dataset) # 予測の実行\n    preds_test[fold] = pred_test[:,0] # 予測結果の代入\nprint(\"preds_test.shape: {}\".format(preds_test.shape))\nprint(preds_test)","metadata":{}},{"cell_type":"markdown","source":"### submitファイルを作成\npreds_test_mean = preds_test.mean(axis=0) # 各foldのmodelの予測の平均値を最終的な予測結果として採用する\nprint(\"preds_test_mean.shape: {}\".format(preds_test_mean.shape))\ndf_sub[col_target] = preds_test.mean(axis=0) # 推定結果を代入\ndf_sub.to_csv(\"submission.tf.csv\", index=None) # submitファイルを保存\ndf_sub.head() # 最初の5行を表示","metadata":{}},{"cell_type":"markdown","source":"## pytorch","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom tqdm import tqdm_notebook as tqdm\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"input_size = 700\n\nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, data, label):\n        super().__init__()\n        \n        self.data = data\n        self.label = label\n        self.len = data.shape[0]\n        \n    def __len__(self):\n        return self.len\n    \n    def __getitem__(self, index):\n        out_data = self.data\n        out_data = out_data[index]\n\n        start_idx = np.random.randint(0,800-input_size-1)\n        out_data = out_data[start_idx:start_idx+input_size:,:]\n        out_label = self.label[index]\n        \n        return out_data, out_label","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Net1D(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv1d(12, 64, kernel_size=7, stride=1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool1d(2)\n\n        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=2)\n        self.bn2 = nn.BatchNorm1d(128)\n        \n        self.conv3 = nn.Conv1d(128,256,kernel_size=3, stride=2)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.gap = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Linear(256,1)\n\n\n    def forward(self,x):\n#         s1, s2, s3 = x.shape\n#         x = x.reshape(s1, s3, s2)\n        x = x.permute(0, 2, 1)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n        x = self.gap(x)\n        x = x.view(x.size(0),-1)\n        x = self.fc(x)\n        x = x.view(-1)\n\n        return x","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# https://github.com/eddymina/ECG_Classification_Pytorch/blob/master/ECG_notebook.ipynb\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nprint(\"\"\"\\nA 1D CNN is very effective when you expect to derive interesting features from shorter \n(fixed-length) segments of the overall data set and where the location of the feature \nwithin the segment is not of high relevance.\\n\"\"\")\n\nclass Anomaly_Classifier(nn.Module):\n    def __init__(self, num_classes):\n        super(Anomaly_Classifier, self).__init__()\n    \n        self.conv= nn.Conv1d(in_channels=12, out_channels=32, kernel_size=5,stride=1)\n        \n        self.conv_pad = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5,stride=1,padding=2)\n        self.drop_50 = nn.Dropout(p=0.5)\n\n        self.maxpool = nn.MaxPool1d(kernel_size=5,stride=2) \n\n#         self.dense1 = nn.Linear(32 * 8, 32) \n        self.dense1 = nn.Linear(1280, 32) \n        self.dense2 = nn.Linear(32, 32) \n        \n        self.dense_final = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)\n        \n        residual= self.conv(x)\n      \n        #block1 \n        x = F.relu(self.conv_pad(residual))\n        x = self.conv_pad(x)\n        x+= residual \n        x = F.relu(x)\n        residual = self.maxpool(x) #[512 32 90]\n       \n        #block2\n        x=F.relu(self.conv_pad(residual))\n        x=self.conv_pad(x)\n        x+=residual\n        x= F.relu(x)\n        residual = self.maxpool(x) #[512 32 43]\n        \n        \n        #block3\n        x=F.relu(self.conv_pad(residual))\n        x=self.conv_pad(x)\n        x+=residual\n        x= F.relu(x)\n        residual = self.maxpool(x) #[512 32 20]\n        \n        \n        #block4\n        x=F.relu(self.conv_pad(residual))\n        x=self.conv_pad(x)\n        x+=residual\n        x= F.relu(x)\n        x= self.maxpool(x) #[512 32 8]\n        \n        s1, s2, s3 = x.shape\n        \n        #MLP\n        x = x.view(-1, s2 * s3) #Reshape (current_dim, 32*2)\n#         print(x.shape)\n        \n        x = F.relu(self.dense1(x))\n        #x = self.drop_60(x)\n        x= self.dense2(x)\n        x = self.dense_final(x)\n        x = x.view(-1)\n        return x","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\nA 1D CNN is very effective when you expect to derive interesting features from shorter \n(fixed-length) segments of the overall data set and where the location of the feature \nwithin the segment is not of high relevance.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Net1D()\nprint(model)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(total_params)","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Net1D(\n  (conv1): Conv1d(12, 64, kernel_size=(7,), stride=(1,))\n  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU()\n  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(2,))\n  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(2,))\n  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (gap): AdaptiveAvgPool1d(output_size=1)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n)\n129857\n","output_type":"stream"}]},{"cell_type":"code","source":"cv = 0\nn_splits = 5\noptimizer_name = 'Adam'\nlr = 0.001\nEPOCHS=30\n\nlist_weights = []\nbest_preds_list = []\nvalid_label_list = []\nfor fold in range(n_splits):\n    X_train = ecg_train[folds[fold][0]] # 学習データの入力データを抽出\n    y_train = target_train[folds[fold][0]] # 学習データの正解データを抽出\n    X_valid = ecg_train[folds[fold][1]] # 検証データの入力データを抽出\n    y_valid = target_train[folds[fold][1]] # 検証データの正解データを抽出\n    \n    X_train = torch.FloatTensor(X_train).to(device)\n    y_train = torch.FloatTensor(y_train).to(device)\n    X_valid = torch.FloatTensor(X_valid).to(device)\n    y_valid = torch.FloatTensor(y_valid).to(device)\n\n    dataset = MyDataset(X_train, y_train)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n    \n    dataset_val = MyDataset(X_valid, y_valid)\n    dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=64, shuffle=False)\n\n#     model = Net1D().cuda()\n    model = Anomaly_Classifier(num_classes= 1).to(device)\n    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=lr)\n    criterion = nn.BCEWithLogitsLoss()\n\n    best_auc = 0\n    for e in range(EPOCHS):\n        avg_loss = 0\n        model.train()\n        for i, (data, y_target) in enumerate(dataloader):\n            optimizer.zero_grad()\n            y_pred = model(data)\n            loss = criterion(y_pred, y_target)\n            loss.backward()\n            optimizer.step()\n        \n            avg_loss += loss.item() / len(dataloader)\n\n        model.eval()\n        avg_val_loss = 0.\n        valid_labels = []\n        preds = []\n        with torch.no_grad():\n            for i, (data, y_target) in enumerate(dataloader_val):\n                y_pred = model(data)\n                valloss = criterion(y_pred, y_target)\n                avg_val_loss += valloss.item() / len(dataloader_val)\n                valid_labels.append(y_target.to('cpu').numpy())\n                preds.append(F.sigmoid(y_pred).cpu().numpy())\n        preds = np.concatenate(preds)\n        valid_labels = np.concatenate(valid_labels)\n        val_auc = roc_auc_score(valid_labels,preds[:])    \n\n        if e % 1 == 0:\n            print('E {}: train loss: {} val loss: {} val AUC: {}'.format(\n                e, avg_loss, avg_val_loss, val_auc))\n\n        if best_auc < val_auc:\n            best_auc = val_auc\n            best_preds = preds\n            print(f'  Epoch {e} - Save Best AUC: {best_auc:.4f}')\n            best_weight = model.state_dict()\n\n    list_weights.append(best_weight)\n    best_preds_list.append(best_preds)\n    valid_label_list.append(valid_labels)\n    \n## calc oof\nbest_preds_list = np.concatenate(best_preds_list)\nvalid_label_list = np.concatenate(valid_label_list)\noof_auc = roc_auc_score(valid_labels,preds[:]) \nprint(f\"OOF_AUC{oof_auc}\")","metadata":{"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"E 0: train loss: 0.5456686675548553 val loss: 0.4652821251324245 val AUC: 0.8388690476190476\n  Epoch 0 - Save Best AUC: 0.8389\nE 1: train loss: 0.3752699881792069 val loss: 0.3131308172430311 val AUC: 0.9252083333333334\n  Epoch 1 - Save Best AUC: 0.9252\nE 2: train loss: 0.3079521864652634 val loss: 0.2848109432629176 val AUC: 0.9379761904761904\n  Epoch 2 - Save Best AUC: 0.9380\nE 3: train loss: 0.27661573618650437 val loss: 0.2944299408367702 val AUC: 0.946547619047619\n  Epoch 3 - Save Best AUC: 0.9465\nE 4: train loss: 0.27404529035091396 val loss: 0.23896477477891107 val AUC: 0.9548214285714286\n  Epoch 4 - Save Best AUC: 0.9548\nE 5: train loss: 0.24106124222278594 val loss: 0.24516976518290384 val AUC: 0.9544345238095238\nE 6: train loss: 0.25626758694648744 val loss: 0.3515859970024654 val AUC: 0.9516071428571429\nE 7: train loss: 0.2405832973122597 val loss: 0.2486893619809832 val AUC: 0.9522619047619048\nE 8: train loss: 0.2084369659423829 val loss: 0.24258463297571456 val AUC: 0.9590773809523809\n  Epoch 8 - Save Best AUC: 0.9591\nE 9: train loss: 0.20182205498218533 val loss: 0.2414559381348746 val AUC: 0.9577380952380953\nE 10: train loss: 0.1954314720630646 val loss: 0.23988625832966393 val AUC: 0.9549702380952381\nE 11: train loss: 0.1782835218310356 val loss: 0.26149916223117287 val AUC: 0.954672619047619\nE 12: train loss: 0.2064143547415733 val loss: 0.255767537014825 val AUC: 0.9527678571428572\nE 13: train loss: 0.167835613489151 val loss: 0.25749131185667856 val AUC: 0.9567261904761905\nE 14: train loss: 0.15490668594837192 val loss: 0.24040743495736802 val AUC: 0.9591071428571427\n  Epoch 14 - Save Best AUC: 0.9591\nE 15: train loss: 0.1436369159817696 val loss: 0.2319434317094939 val AUC: 0.9612797619047619\n  Epoch 15 - Save Best AUC: 0.9613\nE 16: train loss: 0.13952113464474677 val loss: 0.21465527372700827 val AUC: 0.9647916666666666\n  Epoch 16 - Save Best AUC: 0.9648\nE 17: train loss: 0.1580139473080635 val loss: 0.22612695928130833 val AUC: 0.961220238095238\nE 18: train loss: 0.1459026750922203 val loss: 0.2199938744306564 val AUC: 0.9601190476190475\nE 19: train loss: 0.12393626824021342 val loss: 0.29677020226206097 val AUC: 0.9625892857142857\nE 20: train loss: 0.1398439478874207 val loss: 0.24285008013248446 val AUC: 0.9669642857142858\n  Epoch 20 - Save Best AUC: 0.9670\nE 21: train loss: 0.18445943921804425 val loss: 0.21323354329381672 val AUC: 0.9653571428571429\nE 22: train loss: 0.12022654607892039 val loss: 0.26664938032627106 val AUC: 0.9619940476190476\nE 23: train loss: 0.11514347687363627 val loss: 0.24139796729598728 val AUC: 0.9614880952380952\nE 24: train loss: 0.10838744372129443 val loss: 0.26436245228563043 val AUC: 0.9626190476190476\nE 25: train loss: 0.12072673276066781 val loss: 0.28628215193748474 val AUC: 0.9560119047619049\nE 26: train loss: 0.09938578553497789 val loss: 0.3180976297174181 val AUC: 0.9615178571428571\nE 27: train loss: 0.09783282324671745 val loss: 0.23589823927198136 val AUC: 0.9647916666666667\nE 28: train loss: 0.0760778622329235 val loss: 0.28570267132350374 val AUC: 0.9633333333333333\nE 29: train loss: 0.09302480697631835 val loss: 0.25031074987990515 val AUC: 0.964404761904762\nE 0: train loss: 0.5368278741836547 val loss: 0.4368193277290889 val AUC: 0.8478869047619048\n  Epoch 0 - Save Best AUC: 0.8479\nE 1: train loss: 0.4029577910900116 val loss: 0.36886552827698843 val AUC: 0.8885119047619047\n  Epoch 1 - Save Best AUC: 0.8885\nE 2: train loss: 0.30095771908760066 val loss: 0.32793319118874414 val AUC: 0.9135714285714286\n  Epoch 2 - Save Best AUC: 0.9136\nE 3: train loss: 0.2630847257375717 val loss: 0.34204037700380596 val AUC: 0.9294940476190475\n  Epoch 3 - Save Best AUC: 0.9295\nE 4: train loss: 0.24376229465007787 val loss: 0.29813623215470997 val AUC: 0.9335416666666666\n  Epoch 4 - Save Best AUC: 0.9335\nE 5: train loss: 0.23596684038639065 val loss: 0.3034049070307187 val AUC: 0.934375\n  Epoch 5 - Save Best AUC: 0.9344\nE 6: train loss: 0.2218451097607613 val loss: 0.29087463659899576 val AUC: 0.9367261904761904\n  Epoch 6 - Save Best AUC: 0.9367\nE 7: train loss: 0.21681602776050568 val loss: 0.2862289654357093 val AUC: 0.9405357142857144\n  Epoch 7 - Save Best AUC: 0.9405\nE 8: train loss: 0.20256444662809373 val loss: 0.2846385410853795 val AUC: 0.942857142857143\n  Epoch 8 - Save Best AUC: 0.9429\nE 9: train loss: 0.20499102234840386 val loss: 0.2942270998443876 val AUC: 0.9477976190476192\n  Epoch 9 - Save Best AUC: 0.9478\nE 10: train loss: 0.20032192528247839 val loss: 0.2874198832682201 val AUC: 0.9441369047619047\nE 11: train loss: 0.1788162508606911 val loss: 0.28206853994301384 val AUC: 0.9491666666666666\n  Epoch 11 - Save Best AUC: 0.9492\nE 12: train loss: 0.17925897479057312 val loss: 0.2821843113218035 val AUC: 0.9495238095238095\n  Epoch 12 - Save Best AUC: 0.9495\nE 13: train loss: 0.17543107092380522 val loss: 0.2976219707301685 val AUC: 0.9471726190476191\nE 14: train loss: 0.18045081883668904 val loss: 0.2948393779141562 val AUC: 0.9433630952380953\nE 15: train loss: 0.16500918328762054 val loss: 0.2760111114808491 val AUC: 0.9482738095238096\nE 16: train loss: 0.16131169170141227 val loss: 0.2775874755212239 val AUC: 0.9511904761904761\n  Epoch 16 - Save Best AUC: 0.9512\nE 17: train loss: 0.14515250340104105 val loss: 0.2671790048480034 val AUC: 0.9541964285714286\n  Epoch 17 - Save Best AUC: 0.9542\nE 18: train loss: 0.1465637019276619 val loss: 0.3657614695174354 val AUC: 0.9300297619047619\nE 19: train loss: 0.1483435758948326 val loss: 0.3077718828405653 val AUC: 0.9455654761904763\nE 20: train loss: 0.13254018455743788 val loss: 0.29529115344796864 val AUC: 0.950595238095238\nE 21: train loss: 0.11221373185515406 val loss: 0.32153608330658506 val AUC: 0.9568452380952381\n  Epoch 21 - Save Best AUC: 0.9568\nE 22: train loss: 0.12647653818130494 val loss: 0.2748786083289555 val AUC: 0.9558630952380952\nE 23: train loss: 0.10618396922945976 val loss: 0.2636280027883393 val AUC: 0.9573809523809523\n  Epoch 23 - Save Best AUC: 0.9574\nE 24: train loss: 0.11900581642985344 val loss: 0.3128351867198944 val AUC: 0.952827380952381\nE 25: train loss: 0.1170569261908531 val loss: 0.4568827322551182 val AUC: 0.9452083333333333\nE 26: train loss: 0.10185732886195181 val loss: 0.2983023652008602 val AUC: 0.9559523809523809\nE 27: train loss: 0.08315900534391403 val loss: 0.32090567690985544 val AUC: 0.9586904761904762\n  Epoch 27 - Save Best AUC: 0.9587\nE 28: train loss: 0.10742769129574298 val loss: 0.3076192608901433 val AUC: 0.950625\nE 29: train loss: 0.09261420339345934 val loss: 0.31402394601276945 val AUC: 0.9536904761904762\nE 0: train loss: 0.5892471981048583 val loss: 0.5191237756184169 val AUC: 0.8109821428571429\n  Epoch 0 - Save Best AUC: 0.8110\nE 1: train loss: 0.479759989976883 val loss: 0.4520693804536547 val AUC: 0.884345238095238\n  Epoch 1 - Save Best AUC: 0.8843\nE 2: train loss: 0.4102805531024933 val loss: 0.36180716753005976 val AUC: 0.9446428571428571\n  Epoch 2 - Save Best AUC: 0.9446\nE 3: train loss: 0.3974700379371643 val loss: 0.3535339363983699 val AUC: 0.9424404761904762\nE 4: train loss: 0.35864493966102595 val loss: 0.3516800531319209 val AUC: 0.9351190476190476\nE 5: train loss: 0.33290823698043825 val loss: 0.29774458493505207 val AUC: 0.9603422619047619\n  Epoch 5 - Save Best AUC: 0.9603\nE 6: train loss: 0.3076777124404907 val loss: 0.37890020864350454 val AUC: 0.956577380952381\nE 7: train loss: 0.3176777780055999 val loss: 0.2829658750976835 val AUC: 0.9611309523809524\n  Epoch 7 - Save Best AUC: 0.9611\nE 8: train loss: 0.3005541789531707 val loss: 0.253717880163874 val AUC: 0.9680059523809523\n  Epoch 8 - Save Best AUC: 0.9680\nE 9: train loss: 0.26937212228775026 val loss: 0.24770802046571458 val AUC: 0.9676785714285714\nE 10: train loss: 0.2553536891937256 val loss: 0.22786321810313637 val AUC: 0.9718154761904761\n  Epoch 10 - Save Best AUC: 0.9718\nE 11: train loss: 0.2366254901885986 val loss: 0.23820947323526656 val AUC: 0.9677976190476192\nE 12: train loss: 0.22976416707038877 val loss: 0.26947441697120667 val AUC: 0.967470238095238\nE 13: train loss: 0.21220501244068143 val loss: 0.2266845554113388 val AUC: 0.9650297619047619\nE 14: train loss: 0.21483108103275295 val loss: 0.22797915978091107 val AUC: 0.9688690476190476\nE 15: train loss: 0.18935327768325808 val loss: 0.26001616460936405 val AUC: 0.9556547619047618\nE 16: train loss: 0.1933447566628456 val loss: 0.25809694400855476 val AUC: 0.9502083333333333\nE 17: train loss: 0.17855228513479235 val loss: 0.22414168502603257 val AUC: 0.9638244047619048\nE 18: train loss: 0.17439887344837193 val loss: 0.24329870087759833 val AUC: 0.9636309523809524\nE 19: train loss: 0.14779170244932174 val loss: 0.2803263600383486 val AUC: 0.9583035714285714\nE 20: train loss: 0.13253359511494633 val loss: 0.22304930005754742 val AUC: 0.9751488095238096\n  Epoch 20 - Save Best AUC: 0.9751\nE 21: train loss: 0.14914831846952437 val loss: 0.22443510272673195 val AUC: 0.9614285714285714\nE 22: train loss: 0.13622568190097809 val loss: 0.19549972883292607 val AUC: 0.970922619047619\nE 23: train loss: 0.14435006976127623 val loss: 0.19515236360686167 val AUC: 0.9708333333333333\nE 24: train loss: 0.10796509355306624 val loss: 0.22839645934956415 val AUC: 0.965\nE 25: train loss: 0.10740915134549138 val loss: 0.2119298976446901 val AUC: 0.9700892857142858\nE 26: train loss: 0.11107534497976304 val loss: 0.2174628781420844 val AUC: 0.9657886904761905\nE 27: train loss: 0.11586492508649826 val loss: 0.19949777105024882 val AUC: 0.9694196428571429\nE 28: train loss: 0.09902933813631534 val loss: 0.2579988752092634 val AUC: 0.966547619047619\nE 29: train loss: 0.11655701130628586 val loss: 0.3219972550868988 val AUC: 0.9673809523809523\nE 0: train loss: 0.5833285999298097 val loss: 0.5120345226355961 val AUC: 0.8574404761904763\n  Epoch 0 - Save Best AUC: 0.8574\nE 1: train loss: 0.45419602751731875 val loss: 0.30745234233992436 val AUC: 0.9163392857142857\n  Epoch 1 - Save Best AUC: 0.9163\nE 2: train loss: 0.38240372061729433 val loss: 0.30925893996443066 val AUC: 0.9438095238095238\n  Epoch 2 - Save Best AUC: 0.9438\nE 3: train loss: 0.3470089811086655 val loss: 0.25256008761269705 val AUC: 0.9493452380952381\n  Epoch 3 - Save Best AUC: 0.9493\nE 4: train loss: 0.2883053162693977 val loss: 0.2359886701617922 val AUC: 0.9499107142857143\n  Epoch 4 - Save Best AUC: 0.9499\nE 5: train loss: 0.26048597097396853 val loss: 0.22168762449707305 val AUC: 0.9552380952380952\n  Epoch 5 - Save Best AUC: 0.9552\nE 6: train loss: 0.23836646646261214 val loss: 0.22687555370586257 val AUC: 0.9563392857142857\n  Epoch 6 - Save Best AUC: 0.9563\nE 7: train loss: 0.23777584314346312 val loss: 0.22847016155719757 val AUC: 0.9517261904761904\nE 8: train loss: 0.259952680170536 val loss: 0.21124887040683196 val AUC: 0.962202380952381\n  Epoch 8 - Save Best AUC: 0.9622\nE 9: train loss: 0.2421002760529518 val loss: 0.21691626416785378 val AUC: 0.9570238095238096\nE 10: train loss: 0.21482686936855314 val loss: 0.21570743726832525 val AUC: 0.9591666666666667\nE 11: train loss: 0.20455898344516749 val loss: 0.18975672125816342 val AUC: 0.9665178571428571\n  Epoch 11 - Save Best AUC: 0.9665\nE 12: train loss: 0.1990748390555382 val loss: 0.2479995254959379 val AUC: 0.9628869047619047\nE 13: train loss: 0.2010809963941574 val loss: 0.2046605093138559 val AUC: 0.967797619047619\n  Epoch 13 - Save Best AUC: 0.9678\nE 14: train loss: 0.1853033763170242 val loss: 0.20653890499046867 val AUC: 0.9644047619047619\nE 15: train loss: 0.17287280529737473 val loss: 0.2520250486476081 val AUC: 0.9657738095238095\nE 16: train loss: 0.1727221694588661 val loss: 0.19982059938567026 val AUC: 0.9708333333333333\n  Epoch 16 - Save Best AUC: 0.9708\nE 17: train loss: 0.16240183264017102 val loss: 0.2167229365025248 val AUC: 0.9595535714285713\nE 18: train loss: 0.16751719221472738 val loss: 0.18784458935260773 val AUC: 0.969672619047619\nE 19: train loss: 0.19011741548776626 val loss: 0.17898638546466827 val AUC: 0.9690773809523808\nE 20: train loss: 0.15534581005573272 val loss: 0.19164483089532172 val AUC: 0.9672619047619047\nE 21: train loss: 0.14266621708869934 val loss: 0.24935990838067873 val AUC: 0.9639880952380953\nE 22: train loss: 0.16367239683866502 val loss: 0.24270501307078768 val AUC: 0.97125\n  Epoch 22 - Save Best AUC: 0.9712\nE 23: train loss: 0.12811717599630357 val loss: 0.1957186481782368 val AUC: 0.9688690476190476\nE 24: train loss: 0.11935249611735345 val loss: 0.1890538281628064 val AUC: 0.9719345238095237\n  Epoch 24 - Save Best AUC: 0.9719\nE 25: train loss: 0.12071157589554789 val loss: 0.20622578476156506 val AUC: 0.9697619047619047\nE 26: train loss: 0.1181381547451019 val loss: 0.1888556522982461 val AUC: 0.9698511904761904\nE 27: train loss: 0.12539004273712637 val loss: 0.203514610018049 val AUC: 0.96375\nE 28: train loss: 0.10591609984636308 val loss: 0.2841219742383275 val AUC: 0.9614285714285714\nE 29: train loss: 0.15116311520338055 val loss: 0.20405111887625285 val AUC: 0.9704464285714285\nE 0: train loss: 0.5949146711826325 val loss: 0.4883313689913069 val AUC: 0.8060119047619047\n  Epoch 0 - Save Best AUC: 0.8060\nE 1: train loss: 0.5034508740901947 val loss: 0.43165161779948646 val AUC: 0.8644047619047619\n  Epoch 1 - Save Best AUC: 0.8644\nE 2: train loss: 0.3979446029663086 val loss: 0.3616878709622792 val AUC: 0.8859821428571427\n  Epoch 2 - Save Best AUC: 0.8860\nE 3: train loss: 0.2987663596868515 val loss: 0.3357624901192529 val AUC: 0.9168154761904762\n  Epoch 3 - Save Best AUC: 0.9168\nE 4: train loss: 0.27769135177135473 val loss: 0.3045235382659095 val AUC: 0.9181547619047619\n  Epoch 4 - Save Best AUC: 0.9182\nE 5: train loss: 0.25649560153484346 val loss: 0.3218714850289481 val AUC: 0.9199107142857144\n  Epoch 5 - Save Best AUC: 0.9199\nE 6: train loss: 0.23422163516283032 val loss: 0.3346743498529707 val AUC: 0.9199107142857144\nE 7: train loss: 0.21641037225723267 val loss: 0.3151166098458426 val AUC: 0.9152083333333334\nE 8: train loss: 0.2059931960701942 val loss: 0.30565275251865387 val AUC: 0.9254464285714287\n  Epoch 8 - Save Best AUC: 0.9254\nE 9: train loss: 0.18928851813077927 val loss: 0.30474517760532244 val AUC: 0.9272916666666666\n  Epoch 9 - Save Best AUC: 0.9273\nE 10: train loss: 0.18752261847257615 val loss: 0.30601532757282257 val AUC: 0.9219940476190476\nE 11: train loss: 0.1763757503032684 val loss: 0.308282016643456 val AUC: 0.9245238095238095\nE 12: train loss: 0.16696279764175412 val loss: 0.316219619342259 val AUC: 0.9279761904761905\n  Epoch 12 - Save Best AUC: 0.9280\nE 13: train loss: 0.17540713578462602 val loss: 0.31267848717314856 val AUC: 0.9319940476190476\n  Epoch 13 - Save Best AUC: 0.9320\nE 14: train loss: 0.16495406955480577 val loss: 0.31017535073416574 val AUC: 0.9355059523809524\n  Epoch 14 - Save Best AUC: 0.9355\nE 15: train loss: 0.15476258844137192 val loss: 0.31757371340479174 val AUC: 0.9320535714285714\nE 16: train loss: 0.14709842681884763 val loss: 0.3591860226754631 val AUC: 0.9281547619047619\nE 17: train loss: 0.13994995415210723 val loss: 0.31871878142867766 val AUC: 0.934107142857143\nE 18: train loss: 0.13032579198479652 val loss: 0.3803732119766729 val AUC: 0.9310714285714285\nE 19: train loss: 0.14244948670268057 val loss: 0.34103379797722616 val AUC: 0.9302976190476191\nE 20: train loss: 0.11600991159677505 val loss: 0.32140007774744717 val AUC: 0.940595238095238\n  Epoch 20 - Save Best AUC: 0.9406\nE 21: train loss: 0.1323483517765999 val loss: 0.3431125357747078 val AUC: 0.9334821428571428\nE 22: train loss: 0.1345162107050419 val loss: 0.3370675082717623 val AUC: 0.9345535714285715\nE 23: train loss: 0.10119298383593561 val loss: 0.36742020890648874 val AUC: 0.9346428571428571\nE 24: train loss: 0.10459265053272246 val loss: 0.3800772492374693 val AUC: 0.9324107142857143\nE 25: train loss: 0.0952605662122369 val loss: 0.34831198358110016 val AUC: 0.9435119047619047\n  Epoch 25 - Save Best AUC: 0.9435\nE 26: train loss: 0.08302696071565152 val loss: 0.4142094225223576 val AUC: 0.9386607142857142\nE 27: train loss: 0.11603608846664427 val loss: 0.36457169268812456 val AUC: 0.9348809523809524\nE 28: train loss: 0.09242361143231391 val loss: 0.34720473896179876 val AUC: 0.9429464285714285\nE 29: train loss: 0.07384961396455766 val loss: 0.37525138578244616 val AUC: 0.941220238095238\nOOF_AUC0.941220238095238\n","output_type":"stream"}]},{"cell_type":"code","source":"preds_test = np.zeros([n_splits, len(df_test)], np.float32) # 予測結果の代入先\nfor fold, w in tqdm(enumerate(list_weights)):\n#     model = Anomaly_Classifier(input_size=12, num_classes= 1).to(device)\n    list_test = []\n\n    X_test = torch.FloatTensor(ecg_test).to(device)\n\n    dataset_test = MyDataset(X_test, np.zeros(X_test.shape[0]))\n    dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=64, shuffle=False)\n\n#     model = Net1D().to(device)\n    model = Anomaly_Classifier(num_classes= 1).to(device)\n    model.load_state_dict(w)\n    model.eval()\n    l_p = []\n    with torch.no_grad():\n        for i, (data, y_target) in enumerate(dataloader_test):\n            y_p = model(data)\n            y_p = F.sigmoid(y_p).cpu().numpy()\n            l_p.append(y_p)\n    y_pred = np.concatenate(l_p)\n    preds_test[fold] = y_pred","metadata":{"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"662a010a3f944cc2b297168c268f1c83"}},"metadata":{}}]},{"cell_type":"code","source":"import seaborn as sns\n%matplotlib inline","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(preds_test)","metadata":{"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"### submitファイルを作成\npreds_test_mean = preds_test.mean(axis=0) # 各foldのmodelの予測の平均値を最終的な予測結果として採用する\nprint(\"preds_test_mean.shape: {}\".format(preds_test_mean.shape))\ndf_sub[col_target] = preds_test.mean(axis=0) # 推定結果を代入\n# df_sub[col_target] = preds_test[4]\ndf_sub.to_csv(\"submission.torch.csv\", index=None) # submitファイルを保存\ndf_sub.head() # 最初の5行を表示","metadata":{"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"preds_test_mean.shape: (8000,)\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                                 Id    target\n0  eee45832964560ae45040cbc95a252e7  0.033232\n1  6a9adde92c964bd844ddeb12bf559130  0.818206\n2  ee1f947df169cbdc8569a6959913c4ef  0.001048\n3  611df0e51c4fcc5fd1a03887b031a2dc  0.036655\n4  2aeae75bd1d6c3ad42574b68d4daf07c  0.844270","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>eee45832964560ae45040cbc95a252e7</td>\n      <td>0.033232</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6a9adde92c964bd844ddeb12bf559130</td>\n      <td>0.818206</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ee1f947df169cbdc8569a6959913c4ef</td>\n      <td>0.001048</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>611df0e51c4fcc5fd1a03887b031a2dc</td>\n      <td>0.036655</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2aeae75bd1d6c3ad42574b68d4daf07c</td>\n      <td>0.844270</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}